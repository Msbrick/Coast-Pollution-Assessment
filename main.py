import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
warnings.filterwarnings('ignore')

class ShorePollutionPredictor:
    """
    í•´ì•ˆ ì˜¤ì—¼ ì˜ˆì¸¡ ëª¨ë¸ í´ë˜ìŠ¤
   
    ì´ ëª¨ë¸ì€ í•´ì•ˆì˜ ìƒë¬¼í•™ì , í™”í•™ì , ë¬¼ë¦¬ì  ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ
    ì˜¤ì—¼ ìˆ˜ì¤€(0: ë‚®ìŒ, 1: ë³´í†µ, 2: ë†’ìŒ)ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
   
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.scaler = StandardScaler()
        self.feature_columns = None
        self.feature_importance = None
        self.is_trained = False
       
    def load_data(self, filepath):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        self.data = pd.read_csv(filepath)
        print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ ìƒ˜í”Œ")
        return self.data
   
    def preprocess_data(self):
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        # ì£¼ìš” íŠ¹ì„± ì„ íƒ (ê²°ì¸¡ê°’ì´ ì ê³  ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤)
        self.feature_columns = [
            'Month', 'Season', 'Shore',
            'Mean Number of Nematode species 1 per gram soil',
            'Mean Number of Turbillaria per gram soil',
            'Water pH', 'Soil pH', 'Water Salinity', 'Soil Salinity',
            'Total dissolved solids', 'Conduction', 'ORP'
        ]
       
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        X = self.data[self.feature_columns].copy()
        y = self.data['Pollution Level']
       
        # ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´)
        X = X.fillna(X.median())
       
        return X, y
   
    def train_model(self, test_size=0.2):
        """ëª¨ë¸ í›ˆë ¨"""
        X, y = self.preprocess_data()
       
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )
       
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
       
        # ëª¨ë¸ í›ˆë ¨
        self.model.fit(X_train_scaled, y_train)
       
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = self.model.predict(X_test_scaled)
       
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
       
        self.is_trained = True
       
        # ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
        print("ğŸ¯ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:")
        print(f"ì •í™•ë„: {accuracy_score(y_test, y_pred):.3f}")
        print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
        print(classification_report(y_test, y_pred))
       
        return X_train, X_test, y_train, y_test, y_pred
   
    def predict(self, new_data):
        """ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
       
        # ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
        if isinstance(new_data, dict):
            new_data = pd.DataFrame([new_data])
       
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  ê²°ì¸¡ê°’ ì²˜ë¦¬
        X_new = new_data[self.feature_columns].fillna(self.data[self.feature_columns].median())
        X_new_scaled = self.scaler.transform(X_new)
       
        # ì˜ˆì¸¡
        prediction = self.model.predict(X_new_scaled)
        probability = self.model.predict_proba(X_new_scaled)
       
        return prediction, probability
   
    def plot_data_overview(self):
        """ë°ì´í„° ê°œìš” ì‹œê°í™”"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ì˜¤ì—¼ ìˆ˜ì¤€ë³„ ë¶„í¬', 'í•´ì•ˆë³„ ìƒ˜í”Œ ìˆ˜', 'ê³„ì ˆë³„ ë¶„í¬', 'ì›”ë³„ íŠ¸ë Œë“œ'),
            specs=[[{"type": "bar"}, {"type": "bar"}],
                   [{"type": "bar"}, {"type": "scatter"}]]
        )
       
        # ì˜¤ì—¼ ìˆ˜ì¤€ë³„ ë¶„í¬
        pollution_counts = self.data['Pollution Level'].value_counts().sort_index()
        fig.add_trace(
            go.Bar(x=pollution_counts.index, y=pollution_counts.values,
                   name='ì˜¤ì—¼ ìˆ˜ì¤€', marker_color='lightcoral'),
            row=1, col=1
        )
       
        # í•´ì•ˆë³„ ìƒ˜í”Œ ìˆ˜
        shore_counts = self.data['Shore'].value_counts().sort_index()
        fig.add_trace(
            go.Bar(x=shore_counts.index, y=shore_counts.values,
                   name='í•´ì•ˆë³„', marker_color='lightblue'),
            row=1, col=2
        )
       
        # ê³„ì ˆë³„ ë¶„í¬
        season_counts = self.data['Season'].value_counts().sort_index()
        fig.add_trace(
            go.Bar(x=season_counts.index, y=season_counts.values,
                   name='ê³„ì ˆë³„', marker_color='lightgreen'),
            row=2, col=1
        )
       
        # ì›”ë³„ íŠ¸ë Œë“œ
        monthly_pollution = self.data.groupby('Month')['Pollution Level'].mean()
        fig.add_trace(
            go.Scatter(x=monthly_pollution.index, y=monthly_pollution.values,
                      mode='lines+markers', name='ì›”ë³„ í‰ê·  ì˜¤ì—¼ë„',
                      line=dict(color='orange', width=3)),
            row=2, col=2
        )
       
        fig.update_layout(
            title_text="ğŸŒŠ í•´ì•ˆ ì˜¤ì—¼ ë°ì´í„° ê°œìš”",
            showlegend=False,
            height=600
        )
       
        return fig
   
    def plot_feature_importance(self):
        """íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
       
        fig = px.bar(
            self.feature_importance.head(10),
            x='importance',
            y='feature',
            orientation='h',
            title='ğŸ¯ íŠ¹ì„± ì¤‘ìš”ë„ (Top 10)',
            labels={'importance': 'ì¤‘ìš”ë„', 'feature': 'íŠ¹ì„±'}
        )
       
        fig.update_layout(
            yaxis={'categoryorder': 'total ascending'},
            height=500
        )
       
        return fig
   
    def plot_pollution_heatmap(self):
        """í•´ì•ˆë³„ ì˜¤ì—¼ ìˆ˜ì¤€ íˆíŠ¸ë§µ"""
        # í•´ì•ˆë³„, ê³„ì ˆë³„ í‰ê·  ì˜¤ì—¼ë„
        heatmap_data = self.data.pivot_table(
            values='Pollution Level',
            index='Shore',
            columns='Season',
            aggfunc='mean'
        )
       
        fig = px.imshow(
            heatmap_data,
            title='ğŸ—ºï¸ í•´ì•ˆë³„-ê³„ì ˆë³„ í‰ê·  ì˜¤ì—¼ë„',
            labels=dict(x="ê³„ì ˆ", y="í•´ì•ˆ", color="ì˜¤ì—¼ ìˆ˜ì¤€"),
            color_continuous_scale='Reds'
        )
       
        return fig
   
    def plot_correlation_matrix(self):
        """ì£¼ìš” ë³€ìˆ˜ë“¤ ê°„ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤"""
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ë§Œ ì„ íƒ
        numeric_cols = [
            'Pollution Level', 'Water pH', 'Soil pH',
            'Water Salinity', 'Soil Salinity',
            'Mean Number of Nematode species 1 per gram soil',
            'Mean Number of Turbillaria per gram soil'
        ]
       
        correlation_matrix = self.data[numeric_cols].corr()
       
        fig = px.imshow(
            correlation_matrix,
            title='ğŸ”— ì£¼ìš” ë³€ìˆ˜ë“¤ ê°„ ìƒê´€ê´€ê³„',
            color_continuous_scale='RdBu',
            zmin=-1, zmax=1
        )
       
        return fig
   
    def generate_report(self):
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.is_trained:
            self.train_model()
       
        print("=" * 60)
        print("ğŸŒŠ í•´ì•ˆ ì˜¤ì—¼ ì˜ˆì¸¡ ëª¨ë¸ ë¦¬í¬íŠ¸")
        print("=" * 60)
       
        print(f"\nğŸ“Š ë°ì´í„° ê°œìš”:")
        print(f"â€¢ ì´ ìƒ˜í”Œ ìˆ˜: {len(self.data)}")
        print(f"â€¢ í•´ì•ˆ ìˆ˜: {self.data['Shore'].nunique()}")
        print(f"â€¢ ì¡°ì‚¬ ê¸°ê°„: {self.data['Month'].min()}ì›” ~ {self.data['Month'].max()}ì›”")
       
        print(f"\nâš ï¸ ì˜¤ì—¼ ìˆ˜ì¤€ë³„ ë¶„í¬:")
        for level in sorted(self.data['Pollution Level'].unique()):
            count = (self.data['Pollution Level'] == level).sum()
            print(f"â€¢ ìˆ˜ì¤€ {level}: {count}ê°œ ({count/len(self.data)*100:.1f}%)")
       
        print(f"\nğŸ¯ ìƒìœ„ 5ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, row in self.feature_importance.head(5).iterrows():
            print(f"â€¢ {row['feature']}: {row['importance']:.3f}")
       
        return self.feature_importance

# ì‚¬ìš© ì˜ˆì œ
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ëª¨ë¸ ì´ˆê¸°í™”
    predictor = ShorePollutionPredictor()
   
    # ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”)
    # data = predictor.load_data('Shore_Pollution.csv')
   
    # ëª¨ë¸ í›ˆë ¨
    # predictor.train_model()
   
    # ì‹œê°í™”
    # fig1 = predictor.plot_data_overview()
    # fig1.show()
   
    # fig2 = predictor.plot_feature_importance()
    # fig2.show()
   
    # fig3 = predictor.plot_pollution_heatmap()
    # fig3.show()
   
    # ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡ ì˜ˆì œ
    # new_sample = {
    #     'Month': 8,
    #     'Season': 4,
    #     'Shore': 1,
    #     'Mean Number of Nematode species 1 per gram soil': 5.7,
    #     'Mean Number of Turbillaria per gram soil': 0.15,
    #     'Water pH': 8.0,
    #     'Soil pH': 8.2,
    #     'Water Salinity': 38.5,
    #     'Soil Salinity': 11.0,
    #     'Total dissolved solids': 57500,
    #     'Conduction': 57400,
    #     'ORP': -79.9
    # }
   
    # prediction, probability = predictor.predict(new_sample)
    # print(f"ì˜ˆì¸¡ ê²°ê³¼: ì˜¤ì—¼ ìˆ˜ì¤€ {prediction[0]}")
    # print(f"ì˜ˆì¸¡ í™•ë¥ : {probability[0]}")
   
    # ë¦¬í¬íŠ¸ ìƒì„±
    # predictor.generate_report()

if __name__ == "__main__":
    main()
